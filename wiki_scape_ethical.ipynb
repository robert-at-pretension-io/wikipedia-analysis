{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import for days!\n",
    "Let's pull all of the libraries we'll need  for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will this ever be used... the world will never know\n",
    "import tensorflow as tf\n",
    "\n",
    "# scrape wiki plaintext\n",
    "import wikipedia\n",
    "\n",
    "# to view word freq dist\n",
    "import matplotlib.pyplot\n",
    "import tkinter\n",
    "\n",
    "# store the articles so we don't annoy wikipedia (TODO)\n",
    "# (might change to database storage later)\n",
    "import _pickle as cPickle\n",
    "\n",
    "\n",
    "# commonly used words...\n",
    "import re\n",
    "import os.path\n",
    "import urllib.request\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get a list of common words and put them in one list/set\n",
    "\n",
    "__Todo__\n",
    "* Try Different sizes of the set (only top 1,000 stop words?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/robert/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/robert/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "already downloaded the common words list\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "test = stopwords.words('english')\n",
    "\n",
    "common_words = get_stop_words(language='english', cache=True)\n",
    "\n",
    "fn = '10k_common.txt'\n",
    "\n",
    "# wow, so ethical!\n",
    "if os.path.isfile(fn):\n",
    "    print(\"already downloaded the common words list\")\n",
    "    myfile = open(fn, 'r')\n",
    "    most_common = [line for line in myfile.readlines()]\n",
    "else:\n",
    "    print('ayyyy lemme download that common word list file for you!')\n",
    "    url = 'https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-usa.txt'\n",
    "    urllib.request.urlretrieve(url, filename=fn)\n",
    "    myfile = open(fn, 'r')\n",
    "    most_common = [line.strip() for line in myfile.readlines()]\n",
    "   \n",
    "# Awesome, concat_list contains all of the unique common words\n",
    "\n",
    "concat_list = list(set(s.lower().strip() for s in test + common_words + most_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's download a wiki article\n",
    "\n",
    "__TO DO:__\n",
    "* make a local article caching system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================\n",
      "Successfully downloaded wiki article( math )\n",
      "===================\n",
      " __SAMPLE__:\n",
      " Mathematics (from Greek μάθημα máthēma, “knowledge, study, learning”) is the study of topics such as ...\n"
     ]
    }
   ],
   "source": [
    "my_topic = \"math\"\n",
    "\n",
    "article = wikipedia.page(title=my_topic,  auto_suggest=True, redirect=True, preload=False)\n",
    "\n",
    "#article.contents is text of info on the page\n",
    "\n",
    "print(\"\\n===================\\nSuccessfully downloaded wiki article(\", my_topic ,  \")\\n===================\\n __SAMPLE__:\\n\",article.content[0:100],\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some processesing on the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================\n",
      "The top 20 uncommon words in the article on math are: \n",
      "===================\n",
      "\n",
      "('mathematicians', 22) \n",
      "\n",
      "('topology', 9) \n",
      "\n",
      "('axioms', 8) \n",
      "\n",
      "('rigorous', 8) \n",
      "\n",
      "('theorems', 7) \n",
      "\n",
      "('notation', 6) \n",
      "\n",
      "('axiomatic', 6) \n",
      "\n",
      "('hilbert', 6) \n",
      "\n",
      "('arithmetic', 6) \n",
      "\n",
      "('proofs', 5) \n",
      "\n",
      "('algebraic', 5) \n",
      "\n",
      "('mathematicsedit', 5) \n",
      "\n",
      "('conjecture', 5) \n",
      "\n",
      "('rigor', 5) \n",
      "\n",
      "('formulas', 4) \n",
      "\n",
      "('conjectures', 4) \n",
      "\n",
      "('gödel', 4) \n",
      "\n",
      "('axiomatization', 4) \n",
      "\n",
      "('integers', 4) \n",
      "\n",
      "('aesthetic', 4) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(article.content)\n",
    "\n",
    "must_contain_some_letters = re.compile(\"[a-zA-Z]+\")\n",
    "\n",
    "words = [w.lower() for w in tokens]\n",
    "\n",
    "# remove \"words\" that are only comprised of symbols and numbers only\n",
    "words = list(filter(must_contain_some_letters.match, words))\n",
    "\n",
    "# remove stop words\n",
    "words = [w for w in words if w not in concat_list]\n",
    "\n",
    "# sorts the most frequent words\n",
    "top_10_best_words_omg = []\n",
    "for w in set(words):\n",
    "    top_10_best_words_omg.append((w, words.count(w)))\n",
    "\n",
    "top_10_best_words_omg.sort(key=lambda x: x[1], reverse=True)\n",
    "# sort by the second element in the tuple\n",
    "\n",
    "n = 20\n",
    "print(\"\\n===================\\nThe top\", n ,\"uncommon words in the article on\" , my_topic, \"are: \\n===================\\n\")\n",
    "for i in range(n):\n",
    "    print(top_10_best_words_omg[i],\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
